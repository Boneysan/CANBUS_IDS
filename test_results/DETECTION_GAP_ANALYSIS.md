# CAN-IDS Detection Gap Analysis
**Date:** December 27, 2025  
**Analysis Scope:** All detection methods tested against 16 attack datasets  
**Test Environment:** Raspberry Pi 4, Python 3.11.2, Updated ML models from USB

---

## Executive Summary

Our testing revealed **critical coverage gaps** in the current detection system. While some attack types achieve excellent detection (DoS: 98-99%), others have significant weaknesses (Interval: 33-87% variance, RPM: untested). No single configuration provides comprehensive protection across all attack vectors.

### Overall Coverage Matrix

| Attack Type | Adaptive Rules | Fuzzing Rules | Decision Tree | Ensemble | Best Coverage |
|-------------|----------------|---------------|---------------|----------|---------------|
| **DoS Attacks** | 0% ‚ùå | 0% | 98-99% ‚úÖ | Unknown | Decision Tree |
| **Fuzzing** | 0% ‚ùå | ~90% ‚úÖ | 56.8% ‚ö†Ô∏è | Unknown | Fuzzing Rules |
| **Interval Timing** | 0% ‚ùå | 0% | 33-87% ‚ùå | Unknown | Inconsistent |
| **RPM Manipulation** | Unknown | 0% | Unknown | Unknown | **UNTESTED** |
| **Speed Attacks** | Unknown | 0% | Unknown | Unknown | **UNTESTED** |
| **Replay Attacks** | Unknown | 0% | Unknown | Unknown | **UNTESTED** |
| **Normal Traffic FP** | 9.44% ‚úÖ | 0% ‚úÖ | 25.4% ‚ùå | Unknown | Fuzzing Rules |

**Key Finding:** No single detection method achieves >90% detection across all tested attack types with acceptable FP rates.

---

## Gap 1: Dataset-Specific Adaptive Rules ‚ö†Ô∏è CRITICAL

### Problem Description
Adaptive timing rules were trained on specific vehicle datasets and **cannot generalize** to new traffic patterns. Extensive sigma threshold tuning (2.8‚Üí3.5‚Üí5.0) improved FP rate but failed to restore attack detection on different datasets.

### Test Results

| Configuration | Normal Traffic FP | DoS Detection | Conclusion |
|--------------|-------------------|---------------|------------|
| Original (œÉ=2.8/1.4) | 7.36% | 0% recall | Too strict |
| Relaxed (œÉ=5.0/3.0) | 9.08% | 0% recall | Too loose |
| Moderate (œÉ=3.5/2.0) | 9.44% ‚úÖ | 0% recall ‚ùå | Still fails |

**Test Evidence:**
- `test_data/DoS-1.csv`: 90,169 messages, 9,139 attacks ‚Üí **0 detected** (0% recall)
- `test_data/attack-free-1.csv`: 50,000 messages ‚Üí 4,721 alerts (9.44% FP)
- Timing patterns in test data don't match rule expectations trained on different vehicle

### Impact
- ‚ùå Adaptive rules **unusable** on new vehicles without retraining
- ‚ùå Zero detection on DoS attacks despite 9.44% FP on normal traffic
- ‚ö†Ô∏è Requires vehicle-specific baseline learning (time-consuming, error-prone)

### Root Cause
Timing rules encode expected intervals like `expected_interval: 5.46ms, variance: 3.17ms` that are specific to:
- Training vehicle's CAN bus load
- Specific ECU timing characteristics  
- Dataset collection conditions

When deployed on different vehicle/dataset, normal traffic violates these expectations ‚Üí high FP but misses actual attacks.

### Recommended Action
**DO NOT USE** adaptive timing rules for cross-vehicle deployment. Use universal detection methods (fuzzing patterns, ML trained on diverse datasets).

---

## Gap 2: Inconsistent Interval Attack Detection ‚ùå HIGH SEVERITY

### Problem Description
Decision tree shows **extreme variance** (33-87%) detecting interval timing attacks across different datasets, making it unreliable for this attack class.

### Test Results

| Dataset | Detection Rate | Analysis |
|---------|----------------|----------|
| interval-1 | 87.2% | Good performance |
| interval-2 | 33.8% | Failed to detect 2/3 of attacks |
| **Variance** | **53.4 percentage points** | Unacceptable inconsistency |

**Decision Tree Performance Breakdown:**
```
Test Results (from logs/dec27_test3_decision_tree.log):
- Fuzzing Set 1: 56.8% detection, 52.8% FP
- Fuzzing Set 2: 52.8% detection, 57.2% FP  
- DoS Set 1: 99.3% detection, 98.3% FP
- DoS Set 2: 98.3% detection, 99.3% FP
- Interval Set 1: 87.2% detection, 12.8% FP ‚úÖ
- Interval Set 2: 33.8% detection, 66.2% FP ‚ùå FAILED
- Normal Set 1: 17.7% FP (good)
- Normal Set 2: 33.1% FP (high)

Average: 71.4% detection, 25.4% FP
```

### Impact
- ‚ùå Cannot reliably detect interval timing manipulation attacks
- ‚ö†Ô∏è 66% of attacks in interval-2 dataset went undetected
- ‚ùå Interval attacks could bypass IDS ~30-65% of the time

### Root Cause Analysis
1. **Feature limitation:** 12-feature set may not capture subtle timing patterns
2. **Training imbalance:** Model may have been trained primarily on DoS/fuzzing, not interval attacks
3. **Dataset differences:** Interval-1 vs Interval-2 may have different attack characteristics

### Recommended Action
- Train specialized timing analysis model for interval attacks
- Increase feature engineering for timing anomalies
- Consider WCRT (Worst-Case Response Time) analysis
- Test ensemble detector to see if voting improves consistency

---

## Gap 3: Untested Attack Vectors üî¥ CRITICAL

### Problem Description
**Three major attack types have ZERO test coverage** in our validation. Unknown detection capability represents unacceptable production risk.

### Untested Attack Types

#### 3.1 RPM Manipulation Attacks
- **Datasets:** `rpm-1.csv`, `rpm-2.csv` exist but **NOT TESTED**
- **Risk:** Engine over-revving, damage to powertrain
- **Detection capability:** Unknown
- **Impact:** Could cause physical damage to vehicle

#### 3.2 Speed/Accelerometer Attacks  
- **Datasets:** No dedicated test files identified
- **Risk:** Spoofed speed readings, stability control bypass
- **Detection capability:** Unknown
- **Impact:** Safety-critical system compromise

#### 3.3 Replay Attacks
- **Detection method:** Rule-based `check_replay` exists but never validated
- **Risk:** Captured legitimate messages replayed maliciously
- **Detection capability:** Unknown
- **Impact:** Authentication bypass, unauthorized commands

### Test Coverage Gaps

| Dataset Available | Tested | Detection Rate | Gap Severity |
|-------------------|--------|----------------|--------------|
| DoS-1.csv | ‚úÖ | 98-99% | None |
| DoS-2.csv | ‚úÖ | 100% | None |
| fuzzing-1.csv | ‚úÖ | 56.8% | Medium |
| fuzzing-2.csv | ‚úÖ | 52.8% | Medium |
| interval-1.csv | ‚úÖ | 87.2% | Low |
| interval-2.csv | ‚úÖ | 33.8% | High |
| **rpm-1.csv** | ‚ùå | **Unknown** | **CRITICAL** |
| **rpm-2.csv** | ‚ùå | **Unknown** | **CRITICAL** |
| attack-free-1.csv | ‚úÖ | N/A (FP test) | None |
| attack-free-2.csv | ‚úÖ | N/A (FP test) | None |
| standstill-*.csv | ‚ùå | **Unknown** | High |
| force-neutral-*.csv | ‚ùå | **Unknown** | High |
| accessory-*.csv | ‚ùå | **Unknown** | Medium |

**Coverage:** 6/16 datasets tested (37.5%)

### Impact
- üî¥ **63% of available test data unused**
- üî¥ Critical attack types (RPM manipulation) completely unvalidated
- üî¥ Cannot certify system for production deployment
- ‚ö†Ô∏è Unknown false positive rate on standstill/force-neutral scenarios

### Recommended Action
**IMMEDIATE:** Run comprehensive test suite on all 16 datasets before any production deployment.

---

## Gap 4: High False Positive Rate from ML ‚ö†Ô∏è MEDIUM SEVERITY

### Problem Description
Decision tree ML detector generates **25.4% average FP rate** on normal traffic, which could cause alert fatigue and reduce operator trust in production.

### Test Results

**Decision Tree FP Performance:**
- Normal traffic Set 1: 17.7% FP (1,767/10,000 messages)
- Normal traffic Set 2: 33.1% FP (3,313/10,000 messages)
- **Average: 25.4% FP rate**

**Comparison to Other Methods:**
| Method | FP Rate | Notes |
|--------|---------|-------|
| Fuzzing-only rules | 0.0% ‚úÖ | Universal patterns, no timing assumptions |
| Adaptive rules (tuned) | 9.44% ‚úÖ | Good but dataset-specific |
| Decision tree ML | 25.4% ‚ùå | High but detects attacks rules miss |
| Generic aggressive rules | 90-100% ‚ùå | Unusable |

### Impact
- ‚ö†Ô∏è In 8-hour shift processing 1M messages ‚Üí 254,000 false alerts
- ‚ö†Ô∏è Operator alert fatigue reduces response effectiveness
- ‚ö†Ô∏è May mask real attacks in noise

### Root Cause
1. **Training data imbalance:** Model optimized for attack detection over FP minimization
2. **Conservative threshold:** Favors sensitivity (recall) over precision
3. **Feature overlap:** Normal traffic edge cases resemble attacks

### Mitigation Strategy
Multi-stage filtering to reduce FP before reaching ML detector:
1. **Stage 1:** Fuzzing rules (0% FP) filters obvious patterns
2. **Stage 2:** Whitelist known-good CAN IDs
3. **Stage 3:** ML decision tree on remaining messages

**Expected improvement:** 25.4% ‚Üí 5-10% FP with multi-stage approach

---

## Gap 5: Ensemble Detector Untested üî¥ HIGH PRIORITY

### Problem Description
A 680MB `ensemble_detector.joblib` model exists but **has never been validated** against test datasets. Unknown if ensemble voting improves coverage gaps.

### Available Ensemble Models

| Model File | Size | Type | Status |
|------------|------|------|--------|
| `ensemble_detector.joblib` | 680 MB | EnsembleHybridDetector | ‚ùå Untested |
| `ensemble_crosscheck_auto.joblib` | 965 KB | Unknown | ‚ùå Untested |
| `ensemble_impala.joblib` | 965 KB | Unknown | ‚ùå Untested |
| `ensemble_traverse.joblib` | 965 KB | Unknown | ‚ùå Untested |

**Ensemble Architecture (from inspection):**
```python
Type: EnsembleHybridDetector
Components:
  - ml_detectors: Multiple ML models with weighted voting
  - rule_detector: Rule-based cross-checks
  - timing_detector: WCRT-based timing analysis
  - weights: Per-detector voting weights
```

### Potential Benefits (Untested)
1. **Improved interval detection:** Multiple models may reduce 33-87% variance
2. **Lower FP rate:** Voting threshold could filter single-model false positives
3. **Better generalization:** Ensemble may handle dataset differences better

### Known Limitations
1. **Requires DataFrame input:** Not designed for real-time single-message processing
2. **680 MB model size:** Memory constraints on Raspberry Pi (only 4GB RAM)
3. **Unknown inference speed:** May be too slow for production CAN bus rates

### Impact
- üî¥ Potentially **wasted 680 MB of model storage** if ensemble doesn't improve detection
- ‚ö†Ô∏è May solve interval attack inconsistency but **never validated**
- ‚ö†Ô∏è Unknown if memory/speed tradeoff is acceptable

### Recommended Action
**URGENT:** Test ensemble on all attack types, especially interval-1/interval-2 to validate if voting reduces variance.

---

## Gap 6: No RPM/Speed Attack Coverage üî¥ CRITICAL

### Problem Description
Engine RPM and vehicle speed manipulation attacks are **completely untested** despite having dedicated test datasets available.

### Available But Unused Datasets
- `test_data/rpm-1.csv` - Exists, never tested
- `test_data/rpm-2.csv` - Exists, never tested  
- Speed/accelerometer datasets - May not exist

### Attack Scenarios Not Validated

#### RPM Manipulation
- **Attack vector:** Spoofed tachometer messages to show false RPM
- **Impact:** 
  - Driver misled about engine state
  - Automatic transmission shift logic compromised
  - Engine over-revving beyond redline (physical damage)
- **Detection status:** ‚ùå Unknown

#### Speed Spoofing  
- **Attack vector:** False speed sensor data on CAN bus
- **Impact:**
  - Speedometer shows incorrect speed
  - Cruise control malfunction
  - ABS/stability control disabled or compromised
- **Detection status:** ‚ùå Unknown

### Why This Matters
RPM and speed are **safety-critical parameters**. Manipulation could cause:
1. Physical damage to powertrain (over-revving)
2. Loss of vehicle control (speed/ABS compromise)
3. Driver confusion leading to accidents

**Industry standards (ISO 26262) require validation** of all safety-critical attack vectors before production deployment.

### Root Cause of Gap
Test execution focused on "classic" CAN attack types (DoS, fuzzing, interval) without considering vehicle-specific safety implications.

### Recommended Action
**IMMEDIATE PRIORITY:**
1. Run rpm-1.csv and rpm-2.csv through all detectors
2. Analyze detection rates and FP rates
3. If <90% detection, develop RPM-specific detection rules
4. Validate before ANY production deployment

---

## Gap 7: Performance Under High Load Unknown ‚ö†Ô∏è MEDIUM

### Problem Description
All tests used **sampled data** (5,000-50,000 messages) or short durations. Performance under sustained high CAN bus load (500-1000 Hz for hours) is **unknown**.

### Test Limitations

| Test Type | Messages Tested | Duration | Actual CAN Load | Gap |
|-----------|----------------|----------|-----------------|-----|
| DoS-1 adaptive | 90,169 | 15.3s | 5,874 msg/s | Short duration |
| Attack-free adaptive | 1,952,833 | 243.6s | 8,015 msg/s | 4 minutes only |
| Decision tree tests | 10,000 each | <30s | 588 msg/s | Sample size |
| PCA test | 50,000 | Unknown | 17,322 msg/s | No sustained test |

**Real-world CAN bus:** 500-1000 messages/sec for **hours** during normal driving.

### Unknown Factors
1. **Memory growth:** Does detection state accumulate over hours? 
2. **CPU throttling:** Raspberry Pi 4 thermal throttling after 30+ minutes?
3. **Accuracy drift:** Does detection rate degrade over long sessions?
4. **Alert queue overflow:** Can system handle sustained attack + high normal traffic?

### Impact
- ‚ö†Ô∏è Production deployment may experience degradation not seen in short tests
- ‚ö†Ô∏è Memory leaks could crash system after hours of operation
- ‚ö†Ô∏è Thermal throttling could reduce throughput below CAN bus rate

### Recommended Action
Run 8-hour endurance test with:
- Sustained 1000 msg/s load
- Mixed attack types injected randomly
- Monitor CPU, memory, temperature over time
- Validate detection accuracy doesn't degrade

---

## Summary: Critical Gaps Requiring Action

### üî¥ Showstopper Issues (Block Production)

1. **RPM/Speed attacks untested** - Safety-critical gap
2. **63% of test data unused** - Inadequate validation coverage
3. **Ensemble detector never validated** - 680MB model of unknown value
4. **Interval attack 33-87% variance** - Unreliable detection

### ‚ö†Ô∏è High Priority Issues (Require Mitigation)

5. **Adaptive rules unusable cross-vehicle** - Dataset-specific, 0% recall on new data
6. **25.4% FP rate from ML** - Need multi-stage filtering
7. **No sustained load testing** - Unknown long-term performance

### Recommended Test Plan

**Phase 1: Complete Basic Coverage (2-4 hours)**
- [ ] Test rpm-1.csv, rpm-2.csv on all detectors
- [ ] Test standstill-*.csv, force-neutral-*.csv, accessory-*.csv
- [ ] Validate ensemble detector on all attack types
- [ ] Document detection rates and FP rates for ALL 16 datasets

**Phase 2: Address Interval Detection Gap (4-8 hours)**
- [ ] Investigate interval-1 vs interval-2 differences
- [ ] Test ensemble voting on interval attacks
- [ ] Develop specialized timing analysis if needed
- [ ] Validate >80% consistent detection across both datasets

**Phase 3: Production Readiness (1-2 days)**
- [ ] Implement multi-stage filtering (fuzzing ‚Üí whitelist ‚Üí ML)
- [ ] Run 8-hour endurance test with mixed traffic
- [ ] Validate FP rate <5% on normal traffic
- [ ] Document performance under thermal throttling

**Phase 4: Deployment Configuration**
- [ ] Create vehicle-specific configuration guide
- [ ] Document which detectors work cross-vehicle (fuzzing rules, ensemble)
- [ ] Provide retraining procedure for adaptive rules
- [ ] Define monitoring/alerting thresholds

---

## Recommended Production Configuration (Interim)

Until gaps are addressed, use **conservative multi-stage approach**:

```yaml
Stage 1: Fuzzing-Only Rules (config/rules_fuzzing_only.yaml)
  - All-ones pattern: catches fuzzing attacks
  - Sequential pattern: catches systematic fuzzing
  - High entropy: catches random fuzzing
  Expected: 0% FP, ~90% fuzzing detection

Stage 2: ML Decision Tree (data/models/decision_tree.pkl)
  - DoS attacks: 98-99% detection
  - Remaining fuzzing: +56% detection  
  - Interval attacks: 33-87% (unreliable)
  Expected: 25% FP but Stage 1 pre-filters normal traffic

Stage 3: Manual Review Queue
  - Log all Stage 2 alerts for analysis
  - Human validation of edge cases
  - Feedback loop for model improvement

Expected Combined Performance:
  - DoS: 98-99% ‚úÖ
  - Fuzzing: 95%+ ‚úÖ (rules + ML)
  - Interval: 33-87% ‚ùå (needs improvement)
  - RPM/Speed: Unknown ‚ùå (must test first)
  - FP Rate: 5-10% ‚ö†Ô∏è (acceptable with review queue)
```

**DO NOT DEPLOY** until RPM/speed attack coverage validated.

---

## Conclusion

Current detection system has **significant coverage gaps** that prevent production deployment:
- 37.5% test coverage (6/16 datasets)
- 0% detection on safety-critical RPM attacks (untested)
- 33-87% variance on interval attacks (unreliable)
- 680MB ensemble model never validated (wasted resource?)

**Estimated effort to close gaps:** 3-5 days of focused testing and model tuning.

**Production deployment blockers:**
1. ‚úÖ Fuzzing detection: READY (0% FP, ~90% detection)
2. ‚úÖ DoS detection: READY (98-99% detection via ML)
3. ‚ùå Interval detection: NOT READY (inconsistent 33-87%)
4. ‚ùå RPM/Speed detection: NOT READY (untested)
5. ‚ùå Long-term stability: NOT READY (no endurance testing)

**Next Steps:** Execute Phase 1 test plan to complete basic coverage before considering production deployment.
