"""
Multi-Stage Detection Pipeline for Resource-Constrained Environments

Progressive detection through multiple validation stages optimized for Raspberry Pi:
- Stage 1: Fast ML screening (Isolation Forest)
- Stage 2: Lightweight rule validation (DoS/Fuzzing/Spoofing)
- Stage 3: Deep analysis with ensemble + feature engineering

Key Features:
- Adaptive stage gating (skip unnecessary stages)
- Batch processing for efficiency
- Memory-efficient model loading
- Real-time performance monitoring
- Graceful degradation under high load

Expected Performance on Raspberry Pi 4:
- Throughput: ~1,500 msg/s (vs 400 msg/s single-stage)
- Average latency: ~0.5ms (90% fast path)
- Memory: ~210 MB baseline + 1GB for Stage 3
- Recall improvement: +5-15% over single-stage

Author: Vehicle Models Research Team
Date: October 2025
"""

import numpy as np
import pandas as pd
import joblib
import time
from typing import Dict, List, Tuple, Optional, Any, Union
from collections import defaultdict, deque
from pathlib import Path
import warnings
warnings.filterwarnings('ignore')


class StageMetrics:
    """Performance metrics for a single detection stage."""
    
    def __init__(self, stage_name: str):
        self.stage_name = stage_name
        self.messages_processed = 0
        self.messages_passed = 0
        self.messages_filtered = 0
        self.detections = 0
        self.total_time = 0.0
        self.stage_times = deque(maxlen=1000)  # Last 1000 samples
        
    def record_prediction(self, passed: bool, detected: bool, elapsed_time: float):
        """Record a prediction result."""
        self.messages_processed += 1
        if passed:
            self.messages_passed += 1
        else:
            self.messages_filtered += 1
        if detected:
            self.detections += 1
        self.total_time += elapsed_time
        self.stage_times.append(elapsed_time)
    
    def get_statistics(self) -> Dict[str, Any]:
        """Get stage statistics."""
        return {
            'stage': self.stage_name,
            'processed': self.messages_processed,
            'passed': self.messages_passed,
            'filtered': self.messages_filtered,
            'detections': self.detections,
            'pass_rate': self.messages_passed / max(1, self.messages_processed),
            'detection_rate': self.detections / max(1, self.messages_processed),
            'avg_time_ms': (self.total_time / max(1, self.messages_processed)) * 1000,
            'throughput_msg_s': self.messages_processed / max(0.001, self.total_time),
            'recent_avg_time_ms': (np.mean(self.stage_times) * 1000) if self.stage_times else 0.0
        }
    
    def reset(self):
        """Reset all metrics."""
        self.messages_processed = 0
        self.messages_passed = 0
        self.messages_filtered = 0
        self.detections = 0
        self.total_time = 0.0
        self.stage_times.clear()


class MultiStageDetector:
    """
    Multi-stage detection pipeline with progressive complexity.
    
    Architecture:
    
    Stage 1: Fast Screening (Isolation Forest)
    - Purpose: Filter obvious normal traffic quickly
    - Throughput: ~143K msg/s
    - Expected filtering: 60-70% of messages
    - Latency: 0.007ms per message
    
    Stage 2: Rule Validation (DoS/Fuzzing/Spoofing filters)
    - Purpose: Apply lightweight pattern matching
    - Throughput: ~50K msg/s
    - Expected filtering: Additional 20-30%
    - Latency: 0.02ms per message
    
    Stage 3: Deep Analysis (Ensemble + Feature Engineering)
    - Purpose: Comprehensive analysis of suspicious traffic
    - Throughput: ~400 msg/s on Raspberry Pi
    - Processes: Only 10-15% of traffic
    - Latency: 2-3ms per message
    
    Performance Optimization:
    - Adaptive gating: High-confidence detections skip later stages
    - Batch processing: Process multiple messages efficiently
    - Memory management: Load Stage 3 models on-demand
    - Graceful degradation: Fall back to Stage 1+2 under high load
    """
    
    def __init__(self, 
                 stage1_model=None,
                 stage2_rules=None,
                 stage3_ensemble=None,
                 stage1_threshold: float = 0.0,
                 stage2_threshold: float = 0.5,
                 stage3_threshold: float = 0.7,
                 enable_adaptive_gating: bool = True,
                 enable_load_shedding: bool = True,
                 max_stage3_load: float = 0.2,
                 verbose: bool = False):
        """
        Initialize multi-stage detector.
        
        Parameters:
        -----------
        stage1_model : sklearn model
            Fast screening model (Isolation Forest)
        stage2_rules : dict
            Rule-based detectors {filter_name: detector}
        stage3_ensemble : object
            Deep analysis ensemble detector
        stage1_threshold : float
            Decision threshold for Stage 1 (anomaly score)
        stage2_threshold : float
            Minimum confidence for Stage 2 to pass to Stage 3
        stage3_threshold : float
            Final detection threshold for Stage 3
        enable_adaptive_gating : bool
            Skip stages when high confidence achieved
        enable_load_shedding : bool
            Fall back to lighter stages under high load
        max_stage3_load : float
            Maximum fraction of traffic to Stage 3 (0.2 = 20%)
        verbose : bool
            Print detailed processing information
        """
        self.stage1_model = stage1_model
        self.stage2_rules = stage2_rules or {}
        self.stage3_ensemble = stage3_ensemble
        
        self.stage1_threshold = stage1_threshold
        self.stage2_threshold = stage2_threshold
        self.stage3_threshold = stage3_threshold
        
        self.enable_adaptive_gating = enable_adaptive_gating
        self.enable_load_shedding = enable_load_shedding
        self.max_stage3_load = max_stage3_load
        self.verbose = verbose
        
        # Performance metrics
        self.stage1_metrics = StageMetrics("Stage 1: Fast Screening")
        self.stage2_metrics = StageMetrics("Stage 2: Rule Validation")
        self.stage3_metrics = StageMetrics("Stage 3: Deep Analysis")
        
        # Stage 3 load tracking (rolling window)
        self.stage3_load_window = deque(maxlen=1000)
        
        # Feature names expected by models (basic feature set)
        self.required_features = [
            'arb_id_numeric', 'data_length', 'id_frequency', 'time_delta',
            'id_mean_time_delta', 'id_std_time_delta', 'hour', 'minute', 'second'
        ]
    
    def _stage1_screen(self, X: np.ndarray, indices: np.ndarray) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
        """
        Stage 1: Fast screening with Isolation Forest.
        
        Returns:
        --------
        predictions : np.ndarray (0=normal, 1=attack)
        confidences : np.ndarray (confidence scores)
        pass_to_stage2 : np.ndarray (boolean mask)
        """
        start_time = time.time()
        
        if self.stage1_model is None:
            # No Stage 1 model, pass everything to Stage 2
            predictions = np.zeros(len(X), dtype=int)
            confidences = np.zeros(len(X))
            pass_to_stage2 = np.ones(len(X), dtype=bool)
            elapsed = time.time() - start_time
            for idx in indices:
                self.stage1_metrics.record_prediction(True, False, elapsed / len(X))
            return predictions, confidences, pass_to_stage2
        
        # Predict with Isolation Forest
        # IF returns: 1 for inliers (normal), -1 for outliers (anomaly)
        if_predictions = self.stage1_model.predict(X)
        anomaly_scores = self.stage1_model.score_samples(X)
        
        # Convert: -1 (anomaly) ‚Üí 1 (attack), 1 (normal) ‚Üí 0
        predictions = (if_predictions == -1).astype(int)
        
        # Normalize anomaly scores to [0, 1] confidence
        # More negative score = more anomalous = higher confidence
        confidences = 1.0 / (1.0 + np.exp(anomaly_scores * 5))  # Sigmoid transform
        
        # Adaptive gating: High-confidence normals don't need Stage 2
        # High-confidence attacks can skip to final decision
        if self.enable_adaptive_gating:
            # Pass to Stage 2 only if:
            # 1. Predicted as attack (suspicious)
            # 2. Low confidence in normal prediction
            pass_to_stage2 = (predictions == 1) | (confidences < 0.8)
        else:
            # All suspicious messages go to Stage 2
            pass_to_stage2 = predictions == 1
        
        elapsed = time.time() - start_time
        for idx, passed, detected in zip(indices, pass_to_stage2, predictions):
            self.stage1_metrics.record_prediction(passed, detected, elapsed / len(X))
        
        if self.verbose:
            filtered = np.sum(~pass_to_stage2)
            print(f"  Stage 1: Filtered {filtered}/{len(X)} messages ({filtered/len(X)*100:.1f}%)")
        
        return predictions, confidences, pass_to_stage2
    
    def _stage2_validate(self, X: np.ndarray, indices: np.ndarray, 
                        stage1_predictions: np.ndarray,
                        stage1_confidences: np.ndarray) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
        """
        Stage 2: Rule-based validation.
        
        Returns:
        --------
        predictions : np.ndarray (0=normal, 1=attack)
        confidences : np.ndarray (confidence scores)
        pass_to_stage3 : np.ndarray (boolean mask)
        """
        start_time = time.time()
        
        predictions = stage1_predictions.copy()
        confidences = stage1_confidences.copy()
        
        if not self.stage2_rules:
            # No Stage 2 rules, pass suspicious to Stage 3
            pass_to_stage3 = predictions == 1
            elapsed = time.time() - start_time
            for idx in indices:
                self.stage2_metrics.record_prediction(True, False, elapsed / len(X))
            return predictions, confidences, pass_to_stage3
        
        # Apply each rule-based detector
        rule_votes = []
        rule_confidences = []
        
        for rule_name, rule_detector in self.stage2_rules.items():
            try:
                if hasattr(rule_detector, 'predict'):
                    rule_pred = rule_detector.predict(X)
                    rule_votes.append(rule_pred)
                    
                    # Estimate confidence from prediction
                    rule_conf = np.where(rule_pred == 1, 0.9, 0.1)
                    rule_confidences.append(rule_conf)
            except Exception as e:
                if self.verbose:
                    print(f"  Stage 2: Rule '{rule_name}' failed: {e}")
                continue
        
        if rule_votes:
            # Aggregate rule predictions
            rule_votes = np.array(rule_votes)
            rule_confidences = np.array(rule_confidences)
            
            # Voting: attack if any rule triggers
            rule_predictions = np.max(rule_votes, axis=0)
            rule_avg_confidence = np.mean(rule_confidences, axis=0)
            
            # Combine Stage 1 and Stage 2 predictions
            # If either Stage 1 or rules detect attack, mark as suspicious
            predictions = np.maximum(predictions, rule_predictions)
            
            # Update confidence: take max of Stage 1 and rule confidences
            confidences = np.maximum(confidences, rule_avg_confidence)
        
        # Decide which messages need Stage 3 deep analysis
        if self.enable_adaptive_gating:
            # Pass to Stage 3 if:
            # 1. Predicted as attack AND
            # 2. Confidence is not extremely high (needs refinement)
            pass_to_stage3 = (predictions == 1) & (confidences < 0.95)
        else:
            # All attacks go to Stage 3
            pass_to_stage3 = predictions == 1
        
        # Load shedding: limit Stage 3 load
        if self.enable_load_shedding:
            n_stage3 = np.sum(pass_to_stage3)
            current_load = n_stage3 / len(X)
            
            if current_load > self.max_stage3_load:
                # Only send highest-confidence detections to Stage 3
                stage3_limit = int(len(X) * self.max_stage3_load)
                if n_stage3 > stage3_limit:
                    # Sort by confidence, keep top stage3_limit
                    stage3_indices = np.where(pass_to_stage3)[0]
                    top_indices = stage3_indices[np.argsort(-confidences[stage3_indices])[:stage3_limit]]
                    
                    pass_to_stage3_limited = np.zeros_like(pass_to_stage3)
                    pass_to_stage3_limited[top_indices] = True
                    pass_to_stage3 = pass_to_stage3_limited
                    
                    if self.verbose:
                        print(f"  Stage 2: Load shedding {n_stage3} ‚Üí {stage3_limit} messages")
        
        elapsed = time.time() - start_time
        for idx, passed, detected in zip(indices, pass_to_stage3, predictions):
            self.stage2_metrics.record_prediction(passed, detected, elapsed / len(X))
        
        if self.verbose:
            filtered = np.sum(~pass_to_stage3)
            print(f"  Stage 2: Filtered {filtered}/{len(X)} messages ({filtered/len(X)*100:.1f}%)")
        
        return predictions, confidences, pass_to_stage3
    
    def _stage3_analyze(self, X: np.ndarray, indices: np.ndarray,
                       stage2_predictions: np.ndarray,
                       stage2_confidences: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:
        """
        Stage 3: Deep analysis with ensemble.
        
        Returns:
        --------
        predictions : np.ndarray (0=normal, 1=attack)
        confidences : np.ndarray (confidence scores)
        """
        start_time = time.time()
        
        predictions = stage2_predictions.copy()
        confidences = stage2_confidences.copy()
        
        if self.stage3_ensemble is None:
            # No Stage 3 model, use Stage 2 results
            elapsed = time.time() - start_time
            for idx in indices:
                self.stage3_metrics.record_prediction(False, predictions[idx] == 1, elapsed / len(X))
            return predictions, confidences
        
        # Apply ensemble detector
        try:
            if hasattr(self.stage3_ensemble, 'predict_proba'):
                # Get probability predictions
                ensemble_proba = self.stage3_ensemble.predict_proba(X)[:, 1]
                ensemble_predictions = (ensemble_proba >= self.stage3_threshold).astype(int)
                ensemble_confidences = ensemble_proba
            elif hasattr(self.stage3_ensemble, 'decision_function'):
                # Get decision scores
                ensemble_scores = self.stage3_ensemble.decision_function(X)
                # Normalize to [0, 1]
                ensemble_confidences = 1.0 / (1.0 + np.exp(-ensemble_scores))
                ensemble_predictions = (ensemble_confidences >= self.stage3_threshold).astype(int)
            else:
                # Binary predictions only
                ensemble_predictions = self.stage3_ensemble.predict(X)
                ensemble_confidences = np.where(ensemble_predictions == 1, 0.95, 0.05)
            
            # Final decision: ensemble overrides previous stages
            predictions = ensemble_predictions
            confidences = ensemble_confidences
            
        except Exception as e:
            if self.verbose:
                print(f"  Stage 3: Ensemble failed: {e}")
            # Fall back to Stage 2 results
            pass
        
        elapsed = time.time() - start_time
        for idx, detected in zip(indices, predictions):
            self.stage3_metrics.record_prediction(False, detected == 1, elapsed / len(X))
        
        if self.verbose:
            detections = np.sum(predictions)
            print(f"  Stage 3: Detected {detections}/{len(X)} attacks ({detections/len(X)*100:.1f}%)")
        
        return predictions, confidences
    
    def predict(self, X: np.ndarray) -> np.ndarray:
        """
        Multi-stage prediction.
        
        Parameters:
        -----------
        X : np.ndarray or pd.DataFrame
            Feature matrix
        
        Returns:
        --------
        predictions : np.ndarray (0=normal, 1=attack)
        """
        predictions, _ = self.predict_with_confidence(X)
        return predictions
    
    def predict_with_confidence(self, X: Union[np.ndarray, pd.DataFrame]) -> Tuple[np.ndarray, np.ndarray]:
        """
        Multi-stage prediction with confidence scores.
        
        Parameters:
        -----------
        X : np.ndarray or pd.DataFrame
            Feature matrix
        
        Returns:
        --------
        predictions : np.ndarray (0=normal, 1=attack)
        confidences : np.ndarray (confidence scores 0-1)
        """
        # Convert DataFrame to numpy if needed
        if isinstance(X, pd.DataFrame):
            if all(f in X.columns for f in self.required_features):
                X_array = X[self.required_features].values
            else:
                X_array = X.values
        else:
            X_array = X
        
        n_samples = len(X_array)
        all_indices = np.arange(n_samples)
        
        final_predictions = np.zeros(n_samples, dtype=int)
        final_confidences = np.zeros(n_samples)
        
        if self.verbose:
            print(f"\nüîç Multi-Stage Pipeline Processing {n_samples} messages...")
        
        # Stage 1: Fast Screening
        stage1_preds, stage1_conf, pass_to_stage2 = self._stage1_screen(X_array, all_indices)
        final_predictions[:] = stage1_preds
        final_confidences[:] = stage1_conf
        
        # Stage 2: Rule Validation (only for messages that passed Stage 1)
        if np.any(pass_to_stage2):
            stage2_indices = all_indices[pass_to_stage2]
            X_stage2 = X_array[pass_to_stage2]
            
            stage2_preds, stage2_conf, pass_to_stage3 = self._stage2_validate(
                X_stage2, stage2_indices, 
                stage1_preds[pass_to_stage2],
                stage1_conf[pass_to_stage2]
            )
            
            final_predictions[pass_to_stage2] = stage2_preds
            final_confidences[pass_to_stage2] = stage2_conf
            
            # Stage 3: Deep Analysis (only for messages that passed Stage 2)
            if np.any(pass_to_stage3):
                stage3_indices = stage2_indices[pass_to_stage3]
                X_stage3 = X_stage2[pass_to_stage3]
                
                stage3_preds, stage3_conf = self._stage3_analyze(
                    X_stage3, stage3_indices,
                    stage2_preds[pass_to_stage3],
                    stage2_conf[pass_to_stage3]
                )
                
                # Update final predictions for Stage 3 messages
                final_predictions[stage3_indices] = stage3_preds
                final_confidences[stage3_indices] = stage3_conf
                
                # Track Stage 3 load
                self.stage3_load_window.append(len(stage3_indices) / n_samples)
        
        return final_predictions, final_confidences
    
    def get_stage_statistics(self) -> Dict[str, Any]:
        """Get detailed statistics for all stages."""
        stats = {
            'stage1': self.stage1_metrics.get_statistics(),
            'stage2': self.stage2_metrics.get_statistics(),
            'stage3': self.stage3_metrics.get_statistics(),
            'overall': self._compute_overall_statistics()
        }
        return stats
    
    def _compute_overall_statistics(self) -> Dict[str, Any]:
        """Compute overall pipeline statistics."""
        total_messages = self.stage1_metrics.messages_processed
        if total_messages == 0:
            return {}
        
        # Calculate weighted average latency
        stage1_time = self.stage1_metrics.total_time
        stage2_time = self.stage2_metrics.total_time
        stage3_time = self.stage3_metrics.total_time
        total_time = stage1_time + stage2_time + stage3_time
        
        # Stage load distribution
        stage1_load = 1.0  # All messages go through Stage 1
        stage2_load = self.stage2_metrics.messages_processed / max(1, total_messages)
        stage3_load = self.stage3_metrics.messages_processed / max(1, total_messages)
        
        # Average Stage 3 load (rolling window)
        avg_stage3_load = np.mean(self.stage3_load_window) if self.stage3_load_window else 0.0
        
        return {
            'total_messages': total_messages,
            'total_time_sec': total_time,
            'avg_latency_ms': (total_time / max(1, total_messages)) * 1000,
            'throughput_msg_s': total_messages / max(0.001, total_time),
            'stage1_load': stage1_load,
            'stage2_load': stage2_load,
            'stage3_load': stage3_load,
            'avg_stage3_load': avg_stage3_load,
            'total_detections': self.stage1_metrics.detections + 
                               self.stage2_metrics.detections + 
                               self.stage3_metrics.detections,
            'detection_rate': (self.stage1_metrics.detections + 
                             self.stage2_metrics.detections + 
                             self.stage3_metrics.detections) / max(1, total_messages)
        }
    
    def print_statistics(self):
        """Print formatted statistics."""
        stats = self.get_stage_statistics()
        
        print("\n" + "=" * 80)
        print("MULTI-STAGE PIPELINE STATISTICS")
        print("=" * 80)
        
        for stage_name in ['stage1', 'stage2', 'stage3']:
            if stage_name in stats:
                s = stats[stage_name]
                print(f"\n{s['stage']}:")
                print(f"  Processed: {s['processed']:,} messages")
                print(f"  Passed: {s['passed']:,} ({s['pass_rate']*100:.1f}%)")
                print(f"  Filtered: {s['filtered']:,} ({(1-s['pass_rate'])*100:.1f}%)")
                print(f"  Detections: {s['detections']:,} ({s['detection_rate']*100:.1f}%)")
                print(f"  Avg Time: {s['avg_time_ms']:.4f} ms")
                print(f"  Throughput: {s['throughput_msg_s']:,.0f} msg/s")
        
        if 'overall' in stats and stats['overall']:
            o = stats['overall']
            print(f"\n{'='*80}")
            print("OVERALL PIPELINE PERFORMANCE:")
            print(f"  Total Messages: {o['total_messages']:,}")
            print(f"  Total Time: {o['total_time_sec']:.2f} sec")
            print(f"  Average Latency: {o['avg_latency_ms']:.4f} ms")
            print(f"  Throughput: {o['throughput_msg_s']:,.0f} msg/s")
            print(f"\nStage Load Distribution:")
            print(f"  Stage 1 (Screening): {o['stage1_load']*100:.1f}%")
            print(f"  Stage 2 (Rules): {o['stage2_load']*100:.1f}%")
            print(f"  Stage 3 (Deep): {o['stage3_load']*100:.1f}% (avg: {o['avg_stage3_load']*100:.1f}%)")
            print(f"\nTotal Detections: {o['total_detections']:,} ({o['detection_rate']*100:.2f}%)")
            print("=" * 80)
    
    def reset_statistics(self):
        """Reset all performance metrics."""
        self.stage1_metrics.reset()
        self.stage2_metrics.reset()
        self.stage3_metrics.reset()
        self.stage3_load_window.clear()
    
    def save(self, filepath: str):
        """Save detector configuration."""
        config = {
            'stage1_threshold': self.stage1_threshold,
            'stage2_threshold': self.stage2_threshold,
            'stage3_threshold': self.stage3_threshold,
            'enable_adaptive_gating': self.enable_adaptive_gating,
            'enable_load_shedding': self.enable_load_shedding,
            'max_stage3_load': self.max_stage3_load,
            'required_features': self.required_features
        }
        
        # Save models separately
        save_path = Path(filepath)
        save_path.parent.mkdir(parents=True, exist_ok=True)
        
        joblib.dump({
            'config': config,
            'stage1_model': self.stage1_model,
            'stage2_rules': self.stage2_rules,
            'stage3_ensemble': self.stage3_ensemble
        }, filepath)
        
        print(f"‚úÖ Multi-stage detector saved to {filepath}")
    
    @classmethod
    def load(cls, filepath: str, verbose: bool = False):
        """Load detector from file."""
        data = joblib.load(filepath)
        config = data['config']
        
        detector = cls(
            stage1_model=data['stage1_model'],
            stage2_rules=data['stage2_rules'],
            stage3_ensemble=data['stage3_ensemble'],
            stage1_threshold=config['stage1_threshold'],
            stage2_threshold=config['stage2_threshold'],
            stage3_threshold=config['stage3_threshold'],
            enable_adaptive_gating=config['enable_adaptive_gating'],
            enable_load_shedding=config['enable_load_shedding'],
            max_stage3_load=config['max_stage3_load'],
            verbose=verbose
        )
        
        print(f"‚úÖ Multi-stage detector loaded from {filepath}")
        return detector


def create_default_multistage_detector(
    models_dir: str = 'models',
    enable_adaptive_gating: bool = True,
    enable_load_shedding: bool = True,
    max_stage3_load: float = 0.15,
    verbose: bool = False
) -> MultiStageDetector:
    """
    Create a multi-stage detector with default models.
    
    Parameters:
    -----------
    models_dir : str
        Directory containing saved models
    enable_adaptive_gating : bool
        Enable adaptive stage skipping
    enable_load_shedding : bool
        Enable load shedding for Stage 3
    max_stage3_load : float
        Maximum fraction of traffic to Stage 3 (default: 15%)
    verbose : bool
        Print detailed processing information
    
    Returns:
    --------
    detector : MultiStageDetector
    """
    models_path = Path(models_dir)
    
    # Load Stage 1: Isolation Forest
    stage1_model = None
    if_path = models_path / 'improved_isolation_forest.joblib'
    if if_path.exists():
        stage1_model = joblib.load(if_path)
        print(f"‚úÖ Loaded Stage 1: Isolation Forest from {if_path}")
    else:
        print(f"‚ö†Ô∏è Stage 1 model not found at {if_path}")
    
    # Load Stage 2: Rule-based detectors
    stage2_rules = {}
    rule_path = models_path / 'hybrid_rule_detector.joblib'
    if rule_path.exists():
        rules = joblib.load(rule_path)
        stage2_rules['hybrid_rules'] = rules
        print(f"‚úÖ Loaded Stage 2: Hybrid Rules from {rule_path}")
    else:
        print(f"‚ö†Ô∏è Stage 2 rules not found at {rule_path}")
    
    # Load Stage 3: Ensemble
    stage3_ensemble = None
    ensemble_path = models_path / 'ensemble_detector.joblib'
    if ensemble_path.exists():
        stage3_ensemble = joblib.load(ensemble_path)
        print(f"‚úÖ Loaded Stage 3: Ensemble from {ensemble_path}")
    else:
        print(f"‚ö†Ô∏è Stage 3 ensemble not found at {ensemble_path}")
    
    # Create detector
    detector = MultiStageDetector(
        stage1_model=stage1_model,
        stage2_rules=stage2_rules,
        stage3_ensemble=stage3_ensemble,
        stage1_threshold=0.0,
        stage2_threshold=0.5,
        stage3_threshold=0.7,
        enable_adaptive_gating=enable_adaptive_gating,
        enable_load_shedding=enable_load_shedding,
        max_stage3_load=max_stage3_load,
        verbose=verbose
    )
    
    print(f"\n‚úÖ Multi-stage detector created successfully!")
    print(f"   Adaptive Gating: {'Enabled' if enable_adaptive_gating else 'Disabled'}")
    print(f"   Load Shedding: {'Enabled' if enable_load_shedding else 'Disabled'}")
    print(f"   Max Stage 3 Load: {max_stage3_load*100:.0f}%")
    
    return detector
