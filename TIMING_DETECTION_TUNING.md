# Timing-Based Attack Detection: Statistical Tuning and Implementation

**Date:** December 8, 2025  
**Status:** Validated - 99.99% Recall Achieved  
**Next Step:** Implement Hybrid Detection for Production

---

## Executive Summary

Successfully validated that **timing-based detection works** for CAN bus intrusion detection when properly tuned. Testing on 634K messages (including 15K interval timing attacks) demonstrates:

- ‚úÖ **Detection works:** 99.99% recall with 1-sigma thresholds (caught 15,124/15,125 attacks)
- ‚ö†Ô∏è **High false positives:** 34% FPR with single-interval checking
- ‚úÖ **Solution identified:** Hybrid approach combining extreme violation detection + consecutive violation filtering
- üéØ **Target for 7K architecture:** 2-5% FPR, 99%+ recall, <3 message detection latency

---

## Table of Contents

1. [Background and Problem Statement](#background-and-problem-statement)
2. [Statistical Threshold Analysis](#statistical-threshold-analysis)
3. [Detection Approach Comparison](#detection-approach-comparison)
4. [Hybrid Detection Algorithm](#hybrid-detection-algorithm)
5. [Implementation Plan](#implementation-plan)
6. [Testing Validation](#testing-validation)

---

## 1. Background and Problem Statement

### 1.1 Initial Rule Generation

Generated 84 timing and frequency rules from 3.2M attack-free baseline messages using data-driven approach:

**Process:**
```bash
# Generated rules from Vehicle_Models training data
python3 scripts/generate_rules_from_baseline.py \
  --confidence 0.997 \  # 3-sigma (99.7% coverage)
  --output config/rules_timing_3sigma.yaml
```

**Example learned rule (CAN ID 0x1E9):**
```yaml
- name: Timing Anomaly - CAN ID 0x1E9
  can_id: 489  # 0x1E9
  check_timing: true
  expected_interval: 10.92  # milliseconds
  interval_variance: 17.43  # 3-sigma tolerance
  # Valid range: -6.51ms to 28.35ms (effectively 0-28.35ms)
```

### 1.2 Problem Discovery

**Initial testing with 3-sigma thresholds:**
- Dataset: interval-1.csv (634,191 messages, 15,125 attacks)
- Result: **0% recall** - missed all attacks!
- False Positive Rate: 0.00% (10/619,066 = 0.002%)

**Root cause:** Interval timing attacks inject messages at ~20ms intervals, while normal is ~10.92ms. However, 3-sigma tolerance of ¬±17.43ms means valid range is 0-28.35ms, so 20ms attack intervals fall within acceptable range.

**Attack characteristics:**
```
Normal traffic (CAN ID 0x1E9):
  Timestamp    Interval  Data                 Label
  0.081164     -         8000002440000000     Normal
  0.100309     19.1ms    8000002440000000     Normal
  0.120667     20.4ms    8000002440000000     Normal
  0.140749     20.1ms    8000002440000000     Normal

Attack traffic (same CAN ID):
  1.417667     -         000A000C00060000     Attack
  1.437245     19.6ms    000A000C00060000     Attack
  1.460164     22.9ms    000A000C00060000     Attack
  1.477348     17.2ms    000A000C00060000     Attack
```

**Key insight:** Attack uses ~20ms intervals with identical payload, normal uses varying intervals (~10-20ms) with varying payloads. The attack is **subtly slower** than average but within statistical tolerance.

---

## 2. Statistical Threshold Analysis

### 2.1 Sigma Level Testing

Tested three confidence levels to find optimal detection threshold:

| Sigma | Confidence | Tolerance (¬±ms) | Valid Range | Recall | FPR | Analysis |
|-------|-----------|-----------------|-------------|--------|-----|----------|
| **3œÉ** | 99.7% | 17.43 | 0-28.35ms | **0.00%** | 0.00% | ‚ùå Too loose - misses all attacks |
| **2œÉ** | 95.4% | 11.62 | 0-22.54ms | **0.00%** | 0.00% | ‚ùå Still too loose - 20ms attacks pass |
| **1œÉ** | 68.3% | 5.81 | 5.11-16.73ms | **99.99%** | 34% | ‚úÖ Catches attacks but high FPR |

### 2.2 Why 1-Sigma Works

**CAN ID 0x1E9 example:**
- Normal mean: 10.92ms
- Standard deviation: 5.81ms
- **1-sigma range:** 10.92 ¬± 5.81 = **5.11ms to 16.73ms**
- Attack intervals: **~20ms**
- Result: 20ms > 16.73ms ‚Üí **DETECTED** ‚úì

**Statistical reasoning:**
- 1-sigma covers 68.3% of normal distribution
- Remaining 31.7% are natural outliers (temporarily slow/fast messages)
- Attack with sustained 20ms intervals falls in 31.7% tail
- **Challenge:** Distinguishing attack tail from normal tail

### 2.3 False Positive Analysis

**34% FPR with 1-sigma single-interval checking:**

```python
# Current implementation (simplified)
def _check_timing_violation(rule, can_id, timestamp):
    current_interval = timing_history[-1]
    
    if current_interval < 5.11 or current_interval > 16.73:
        return True  # ALERT on ANY single violation
```

**Why 34% FPR is close to theoretical 32%:**
- 1-sigma means 68.3% of data within range
- 31.7% of data outside range (16% too slow, 16% too fast)
- Observed 34% FPR ‚âà theoretical 32% = correct statistical behavior
- **Problem:** Can't distinguish random outliers from attack outliers

**Example false positive:**
```
Normal traffic with natural jitter:
  Message 1: 11ms ‚úì OK
  Message 2: 11ms ‚úì OK
  Message 3: 13ms ‚úì OK
  Message 4: 17ms ‚ùå ALERT (> 16.73ms threshold)
  Message 5: 11ms ‚úì OK

Single spike at 17ms is normal jitter, not an attack!
```

---

## 3. Detection Approach Comparison

### 3.1 Approach 1: Single Interval Check (Current)

**Algorithm:**
```python
def _check_timing_violation(rule, can_id, timestamp):
    if len(timing_history) < 2:
        return False
    
    current_interval = timing_history[-1]
    min_allowed = rule.expected_interval - rule.interval_variance
    max_allowed = rule.expected_interval + rule.interval_variance
    
    # Alert on ANY violation
    return current_interval < min_allowed or current_interval > max_allowed
```

**Characteristics:**
- ‚úÖ **Instant detection:** Triggers on first attack message
- ‚úÖ **Simple implementation:** Single comparison
- ‚úÖ **99.99% recall:** Catches nearly all attacks
- ‚ùå **34% FPR:** Unacceptable for production (false alarm every 3 messages)

**Best for:** Development, testing, maximum sensitivity

---

### 3.2 Approach 2: Moving Average

**Algorithm:**
```python
def _check_timing_violation_moving_avg(rule, can_id, timestamp):
    if len(timing_history) < 5:
        return False
    
    # Average last 5 intervals
    recent_intervals = timing_history[-5:]
    avg_interval = sum(recent_intervals) / 5
    
    min_allowed = rule.expected_interval - rule.interval_variance
    max_allowed = rule.expected_interval + rule.interval_variance
    
    return avg_interval < min_allowed or avg_interval > max_allowed
```

**Example behavior:**
```
Normal jitter (single spike):
  Intervals: [11, 11, 13, 17, 11]
  Average: 12.6ms ‚úì OK (within 5.11-16.73ms)
  No alert

Sustained attack:
  Intervals: [11, 11, 20, 20, 20]
  Average: 16.4ms ‚úì OK (borderline)
  
  Next message: [11, 20, 20, 20, 20]
  Average: 18.2ms ‚ùå ALERT (> 16.73ms)
```

**Characteristics:**
- ‚úÖ **Lower FPR:** ~5-10% (smooths random jitter)
- ‚úÖ **Simple implementation:** Just calculate average
- ‚ö†Ô∏è **Slower detection:** Requires 3-5 attack messages before average shifts
- ‚ö†Ô∏è **95-99% recall:** May miss brief attacks

**Best for:** Sustained attacks, medium-sensitivity deployments

---

### 3.3 Approach 3: Consecutive Violations

**Algorithm:**
```python
def _check_timing_violation_consecutive(rule, can_id, timestamp):
    if len(timing_history) < 3:
        return False
    
    last_3_intervals = timing_history[-3:]
    min_allowed = rule.expected_interval - rule.interval_variance
    max_allowed = rule.expected_interval + rule.interval_variance
    
    # Count violations in last 3 intervals
    violations = 0
    for interval in last_3_intervals:
        if interval < min_allowed or interval > max_allowed:
            violations += 1
    
    # Alert only if 3 consecutive violations
    return violations >= 3
```

**Example behavior:**
```
Normal jitter (random spikes):
  Intervals: [11, 13, 17, 11, 13]
  Last 3: [17, 11, 13] ‚Üí 1/3 violated ‚úì No alert
  
Sustained attack:
  Intervals: [11, 11, 20, 20, 20]
  Last 3: [20, 20, 20] ‚Üí 3/3 violated ‚ùå ALERT
```

**Statistical reasoning:**
```
Probability of false positive:
  P(single violation) = 32% (1-sigma tail)
  P(3 consecutive) = 0.32¬≥ = 3.3%
  
Expected FPR: ~1-3% ‚úì Matches 7K architecture target!
```

**Characteristics:**
- ‚úÖ **Very low FPR:** ~1-3% (only random chance creates 3 consecutive spikes)
- ‚úÖ **Fast detection:** 3 messages (~30-60ms for high-traffic IDs)
- ‚úÖ **95-99% recall:** Catches sustained attacks
- ‚ö†Ô∏è **May miss single injections:** Brief attacks might not create 3 consecutive violations

**Best for:** Production systems, Stage 1 of 7K architecture

---

### 3.4 Approach 4: Hybrid (Recommended)

**Algorithm:**
```python
def _check_timing_violation_hybrid(rule, can_id, timestamp):
    """
    Hybrid detection combining immediate extreme violation detection
    with consecutive violation filtering for moderate anomalies.
    
    Two-tier approach:
    1. Extreme violations (>2-sigma): Immediate alert (high confidence)
    2. Moderate violations (1-2 sigma): Require 3 consecutive (filter jitter)
    """
    if len(timing_history) < 2:
        return False
    
    current_interval = timing_history[-1]
    expected = rule.expected_interval
    variance_1sigma = rule.interval_variance  # 1-sigma tolerance
    variance_2sigma = variance_1sigma * 2     # 2-sigma tolerance
    
    # Tier 1: Extreme violation detection (immediate alert)
    # These are almost certainly attacks, not normal jitter
    extreme_min = expected - variance_2sigma
    extreme_max = expected + variance_2sigma
    
    if current_interval < extreme_min or current_interval > extreme_max:
        return True  # Immediate alert - very far from normal
    
    # Tier 2: Moderate violation detection (require consecutive)
    # These could be jitter, require 3 in a row for confidence
    if len(timing_history) >= 3:
        moderate_min = expected - variance_1sigma
        moderate_max = expected + variance_1sigma
        
        last_3_intervals = timing_history[-3:]
        violations = 0
        for interval in last_3_intervals:
            if interval < moderate_min or interval > moderate_max:
                violations += 1
        
        if violations >= 3:
            return True  # 3 consecutive moderate violations
    
    return False
```

**Example behavior with CAN ID 0x1E9:**

**Configuration:**
- Expected: 10.92ms
- 1-sigma: ¬±5.81ms (range: 5.11-16.73ms)
- 2-sigma: ¬±11.62ms (range: 0-22.54ms)

**Scenario 1: Normal jitter (single spike)**
```
Intervals: [11, 11, 13, 17, 11]
Message at 17ms:
  - Not extreme (< 22.54ms) ‚úì
  - Last 3: [13, 17, 11] ‚Üí only 1/3 violated ‚úì
  - No alert ‚úì
```

**Scenario 2: Extreme DoS attack (1ms flooding)**
```
Intervals: [11, 11, 1, 1, 1]
Message at 1ms:
  - Extreme violation! (1ms << 5.11ms minimum)
  - Immediate alert on first 1ms message ‚ùå
  - High confidence attack
```

**Scenario 3: Subtle timing attack (20ms intervals)**
```
Intervals: [11, 11, 20, 20, 20]
First 20ms message:
  - Not extreme (20ms < 22.54ms) ‚úì
  - Last 3: [11, 11, 20] ‚Üí only 1/3 violated ‚úì
  - No alert yet

Second 20ms message:
  - Not extreme ‚úì
  - Last 3: [11, 20, 20] ‚Üí 2/3 violated ‚úì
  - No alert yet

Third 20ms message:
  - Not extreme ‚úì
  - Last 3: [20, 20, 20] ‚Üí 3/3 violated ‚ùå
  - Alert on 3rd attack message
```

**Characteristics:**
- ‚úÖ **Low FPR:** ~2-5% (extreme + 3-consecutive filter)
- ‚úÖ **Fast extreme detection:** 1 message for DoS/flooding
- ‚úÖ **Fast subtle detection:** 3 messages for timing attacks
- ‚úÖ **99%+ recall:** Catches both obvious and subtle attacks
- ‚úÖ **Production-ready:** Balances sensitivity and false alarms

**Best for:** 7K msg/s architecture Stage 1, production deployments

---

## 4. Hybrid Detection Algorithm

### 4.1 Complete Implementation

```python
def _check_timing_violation(self, rule: DetectionRule, can_id: int, timestamp: float) -> bool:
    """
    Check for timing anomalies using hybrid detection approach.
    
    Combines two detection tiers:
    1. Immediate detection for extreme violations (>2-sigma from normal)
    2. Consecutive violation detection for moderate anomalies (1-2 sigma)
    
    This approach minimizes false positives from natural timing jitter while
    maintaining high sensitivity to actual attacks.
    
    Args:
        rule: Detection rule with expected_interval and interval_variance (1-sigma)
        can_id: CAN identifier
        timestamp: Message timestamp
        
    Returns:
        True if timing violation detected, False otherwise
    """
    if not rule.expected_interval or not rule.interval_variance:
        return False
    
    timing_history = self._timing_analysis[can_id]
    
    if len(timing_history) < 2:
        return False  # Need at least 2 messages to calculate interval
    
    current_interval = timing_history[-1]
    expected = rule.expected_interval
    variance_1sigma = rule.interval_variance
    variance_2sigma = variance_1sigma * 2
    
    # Tier 1: Extreme Violation Detection
    # Detects obvious attacks (DoS flooding, severe delays)
    # Example: 1ms intervals when normal is 10ms, or 50ms when normal is 10ms
    extreme_min = max(0, expected - variance_2sigma)
    extreme_max = expected + variance_2sigma
    
    if current_interval < extreme_min:
        # Message arrived WAY too fast (DoS, injection attack)
        logger.debug(
            f"CAN ID 0x{can_id:03X}: Extreme timing violation (too fast) - "
            f"interval={current_interval:.2f}ms < {extreme_min:.2f}ms"
        )
        return True
    
    if current_interval > extreme_max:
        # Message arrived WAY too slow (suspension, delay attack)
        logger.debug(
            f"CAN ID 0x{can_id:03X}: Extreme timing violation (too slow) - "
            f"interval={current_interval:.2f}ms > {extreme_max:.2f}ms"
        )
        return True
    
    # Tier 2: Consecutive Moderate Violation Detection
    # Filters normal timing jitter by requiring sustained pattern change
    if len(timing_history) >= 3:
        moderate_min = max(0, expected - variance_1sigma)
        moderate_max = expected + variance_1sigma
        
        last_3_intervals = timing_history[-3:]
        violations = 0
        for interval in last_3_intervals:
            if interval < moderate_min or interval > moderate_max:
                violations += 1
        
        if violations >= 3:
            # 3 consecutive violations indicates sustained attack, not random jitter
            logger.debug(
                f"CAN ID 0x{can_id:03X}: Consecutive timing violations - "
                f"last_3={[f'{i:.2f}' for i in last_3_intervals]}, "
                f"expected={expected:.2f}¬±{variance_1sigma:.2f}ms"
            )
            return True
    
    return False
```

### 4.2 Performance Characteristics

**Computational Complexity:**
- Time: O(1) - constant time lookups and comparisons
- Space: O(k) per CAN ID, where k = max history length (typically 10-20)
- Throughput: Can process 7,000+ msg/s (timing checks are simple arithmetic)

**Detection Latency:**
| Attack Type | Detection Speed | Example |
|-------------|-----------------|---------|
| **Extreme DoS** (1ms intervals) | 1 message | ~0.001-0.01 seconds |
| **Extreme delay** (>50ms) | 1 message | ~0.05-0.1 seconds |
| **Subtle timing** (20ms vs 10ms normal) | 3 messages | ~0.03-0.06 seconds |
| **Normal jitter** | No alert | N/A |

**False Positive Rate:**
```
Tier 1 (Extreme): ~0.1-0.5%
  - 2-sigma = 95.4% coverage
  - Only 4.6% of normal traffic outside range
  - Almost all of that 4.6% is mild, not extreme
  
Tier 2 (Consecutive): ~3%
  - Single violation: 32% (1-sigma tail)
  - Three consecutive: 0.32¬≥ = 3.3%
  
Combined: ~2-5% FPR
```

**True Positive Rate (Recall):**
```
Sustained attacks (3+ messages): 99%+
Brief attacks (1-2 messages):
  - Extreme: 99%+ (detected immediately)
  - Subtle: 50-70% (may not trigger consecutive)
  
Overall expected recall: 95-99%
```

---

## 5. Implementation Plan

### 5.1 Code Changes

**File:** `src/detection/rule_engine.py`

**Location:** Replace `_check_timing_violation()` method (currently lines 424-450)

**Changes:**
```python
# BEFORE (current single-interval check):
def _check_timing_violation(self, rule, can_id, timestamp):
    if len(timing_history) < 2:
        return False
    current_interval = timing_history[-1]
    min_interval = expected - tolerance
    max_interval = expected + tolerance
    if current_interval < min_interval or current_interval > max_interval:
        return True
    return False

# AFTER (hybrid approach):
def _check_timing_violation(self, rule, can_id, timestamp):
    # Tier 1: Extreme violations (2-sigma)
    if current_interval outside 2-sigma range:
        return True  # Immediate alert
    
    # Tier 2: Consecutive violations (1-sigma, N=3)
    if len(timing_history) >= 3:
        if all last_3_intervals outside 1-sigma range:
            return True
    
    return False
```

### 5.2 Configuration

**Rules:** Use `config/rules_timing_1sigma.yaml` (already generated)

**Key parameters:**
- `expected_interval`: Learned mean from baseline data
- `interval_variance`: 1-sigma standard deviation
- `check_timing: true`: Enable timing checks

**Example rule:**
```yaml
- name: Timing Anomaly - CAN ID 0x1E9
  can_id: 489
  severity: MEDIUM
  description: Message timing deviates from baseline (learned from 123,687 messages)
  action: alert
  check_timing: true
  expected_interval: 10.92
  interval_variance: 5.81  # 1-sigma, auto-calculated as 2-sigma in code
```

### 5.3 Testing Plan

**Phase 1: Validate on known attacks**
```bash
# Test on interval timing attacks (baseline)
python3 scripts/test_rules_on_dataset.py \
  ../Vehicle_Models/data/raw/interval-1.csv \
  --rules config/rules_timing_1sigma.yaml

# Expected results with hybrid:
# - Recall: 95-99% (vs 99.99% with single-interval)
# - FPR: 2-5% (vs 34% with single-interval)
```

**Phase 2: Cross-validate on other attack types**
```bash
# DoS attacks (should trigger Tier 1 - extreme violations)
python3 scripts/test_rules_on_dataset.py \
  ../Vehicle_Models/data/raw/DoS-1.csv \
  --rules config/rules_timing_1sigma.yaml

# Fuzzing attacks (may not be timing-based)
python3 scripts/test_rules_on_dataset.py \
  ../Vehicle_Models/data/raw/fuzzing-1.csv \
  --rules config/rules_timing_1sigma.yaml
```

**Phase 3: False positive validation**
```bash
# Test on attack-free data
python3 scripts/test_rules_on_dataset.py \
  ../Vehicle_Models/data/raw/attack-free-1.csv \
  --rules config/rules_timing_1sigma.yaml

# Expected: <5% FPR on clean data
```

---

## 6. Testing Validation

### 6.1 Initial Results (1-Sigma Single Interval)

**Test Date:** December 8, 2025  
**Dataset:** interval-1.csv (634,191 messages, 15,125 attacks)  
**Configuration:** 1-sigma thresholds, single interval checking

**Results:**
```
Total Alerts: 225,130

Confusion Matrix:
  True Positives:  15,124 (attacks caught)
  False Positives: 210,006 (normal flagged as attack)
  True Negatives:  409,060 (normal correctly ignored)
  False Negatives: 1 (attacks missed)

‚úì Recall: 99.99% (caught 15124/15125 attacks)
‚úì Precision: 6.72% (210,006 false alarms)
‚úó False Positive Rate: 33.92% (unacceptable for production)
```

**Key findings:**
- ‚úÖ **Timing detection works!** 99.99% of attacks caught
- ‚úÖ **1-sigma thresholds are correct** (attack falls in 1-2 sigma range)
- ‚ùå **34% FPR too high** for production use
- ‚úÖ **FPR matches theory** (32% expected for 1-sigma single checks)

### 6.2 Expected Results (Hybrid Approach)

**Prediction based on statistical analysis:**

```
Expected Alerts: ~30,000-40,000 (vs 225,130 current)

Confusion Matrix (estimated):
  True Positives:  14,500-15,000 (95-99% of attacks)
  False Positives: 15,000-25,000 (2-5% of normal traffic)
  True Negatives:  594,000-604,000
  False Negatives: 125-625 (1-5% of attacks)

‚úì Recall: 95-99% (acceptable tradeoff)
‚úì Precision: 40-50% (1 real attack per 1-2 false alarms)
‚úì False Positive Rate: 2-5% (production-ready)
```

**Reasoning:**
- Tier 1 (extreme): Catches ~10% of attacks immediately (very obvious)
- Tier 2 (consecutive): Catches ~85-89% of attacks (sustained patterns)
- Combined recall: 95-99%
- False positives reduced by 10x (from 34% to 2-5%)

### 6.3 Integration with 7K Architecture

**Stage 1 (Cycle Filter) Performance:**

```
Input: 7,000 msg/s
  ‚Üì
Hybrid Timing Detection (84 rules, 1-sigma + consecutive)
  ‚Üì
Pass Rate: 95-98% (2-5% FPR)
  ‚Üì
Output: ~140-350 msg/s flagged ‚Üí Stage 2
```

**Comparison to BUILD_PLAN target:**
- Target pass rate: 80% (1,400 msg/s to Stage 2)
- Hybrid pass rate: 95-98% (140-350 msg/s to Stage 2)
- **Result:** Better than target! Lower false positives = less load on Stage 2

**Impact on overall system:**
- Stage 1 throughput: 7,000+ msg/s ‚úì (timing checks are O(1))
- Stage 2 input: 140-350 msg/s (vs target 1,400)
- Stage 3 input: Further reduced by Stage 2
- **Conclusion:** Hybrid approach exceeds 7K architecture requirements

---

## Appendix A: Statistical Foundation

### Normal Distribution and Sigma Levels

```
Normal distribution of CAN message intervals:

                    |
        1-sigma (68.3%)
    |<----------------->|
         2-sigma (95.4%)
    |<-------------------->|
          3-sigma (99.7%)
    |<---------------------->|
    
    
    ‚îÇ                 ‚ï±‚Äæ‚ï≤
    ‚îÇ               ‚ï±     ‚ï≤
    ‚îÇ             ‚ï±         ‚ï≤
    ‚îÇ           ‚ï±             ‚ï≤
    ‚îÇ         ‚ï±                 ‚ï≤
    ‚îÇ       ‚ï±                     ‚ï≤
    ‚îÇ     ‚ï±                         ‚ï≤
    ‚îÇ   ‚ï±                             ‚ï≤
    ‚îÇ ‚ï±                                 ‚ï≤
    ‚îÇ‚ï±___________________________________‚ï≤___
     Œº-3œÉ  Œº-2œÉ  Œº-1œÉ   Œº   Œº+1œÉ Œº+2œÉ Œº+3œÉ
     
    Attack falls here (20ms) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
    Normal is here (10.92ms) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Probability Calculations

**Single violation probability:**
- 1-sigma: P(outside) = 31.7%
- 2-sigma: P(outside) = 4.6%
- 3-sigma: P(outside) = 0.3%

**Consecutive violation probability (N=3):**
- 1-sigma: P = 0.317¬≥ = 3.2%
- 2-sigma: P = 0.046¬≥ = 0.01%
- 3-sigma: P = 0.003¬≥ = 0.000003%

**Hybrid approach:**
- Extreme (2-sigma immediate): 4.6% - (most is mild) ‚âà 0.5% FPR
- Consecutive (1-sigma √ó 3): 3.2% FPR
- Combined: ~2-5% FPR (accounting for overlaps)

---

## Appendix B: Attack Type Coverage

### Timing-Based Attacks (Covered)

**1. Interval Timing Attacks**
- Injection at slower/faster rate than normal
- Detection: Tier 2 (consecutive violations)
- Recall: 95-99%

**2. DoS/Flooding Attacks**
- Rapid message injection (1-5ms intervals)
- Detection: Tier 1 (extreme violations)
- Recall: 99%+

**3. Suspension Attacks**
- Preventing normal messages (intervals > 50ms)
- Detection: Tier 1 (extreme violations)
- Recall: 99%+

**4. Replay Attacks** (with timing component)
- Replayed messages at wrong timing
- Detection: Tier 2 (consecutive violations)
- Recall: 70-90%

### Non-Timing Attacks (Not Covered)

**1. Fuzzing Attacks**
- Random data payloads at normal timing
- Detection: Requires payload analysis (Stage 2)
- Coverage: 0% from timing alone

**2. Targeted Injection** (single message)
- One malicious message at normal timing
- Detection: Requires payload/semantic analysis
- Coverage: 0-50% (only if timing is off)

**3. MitM Manipulation**
- Modified data values at normal timing
- Detection: Requires payload validation
- Coverage: 0% from timing alone

---

## Appendix C: Per-CAN-ID Adaptive Thresholds

### C.1 Problem: Fixed Thresholds Don't Work Across All CAN IDs

**Testing Results (December 8, 2025):**

```
Hybrid Detection with Fixed 1-sigma + 3-sigma extreme + N=5 consecutive:

Dataset: interval-1.csv (634K messages)
- Recall: 99.69% ‚úì (excellent attack detection)
- FPR: 25.25% ‚ùå (unacceptable for production)

Dataset: attack-free-1.csv (1.95M messages)  
- FPR: 40.54% ‚ùå (worse on different vehicle/conditions)
```

**Root Cause:** Different CAN IDs have vastly different timing characteristics:

| CAN ID | Message Rate | Mean Interval | Std Dev | Variance Ratio |
|--------|-------------|---------------|---------|----------------|
| 0x1E9 | 91.5 msg/s | 10.92ms | 5.81ms | 53% (high jitter) |
| 0x130 | 50.2 msg/s | 19.92ms | 2.43ms | 12% (low jitter) |
| 0x771 | 1.6 msg/s | 606.2ms | 304.2ms | 50% (high jitter) |

**One-size-fits-all thresholds fail:**
- High-jitter CAN IDs (0x1E9): 1-sigma too tight ‚Üí high FPR
- Low-jitter CAN IDs (0x130): 1-sigma perfect ‚Üí but forced to use looser thresholds
- Result: Either high FPR (tight) or missed attacks (loose)

### C.2 Solution: Per-CAN-ID Adaptive Thresholds

**Approach:** Calculate optimal sigma multipliers based on CAN ID characteristics during rule generation.

**Key Insight:** High-traffic CAN IDs have more samples ‚Üí better statistics ‚Üí can use tighter thresholds.

```python
# Adaptive threshold selection algorithm
def calculate_adaptive_thresholds(can_id_stats):
    message_rate = stats['count'] / total_duration
    std_dev = stats['std']
    mean = stats['mean']
    coefficient_of_variation = std_dev / mean  # Normalized jitter
    
    # High-traffic CAN IDs (>50 msg/s):
    # - More samples = better statistical confidence
    # - Can use tighter thresholds
    if message_rate > 50:
        sigma_extreme = 2.5
        sigma_moderate = 1.0
        consecutive_required = 5
    
    # Medium-traffic CAN IDs (10-50 msg/s):
    # - Standard statistical approach
    elif message_rate > 10:
        sigma_extreme = 3.0
        sigma_moderate = 1.0
        consecutive_required = 4
    
    # Low-traffic CAN IDs (<10 msg/s):
    # - Fewer samples = less confidence
    # - Need looser thresholds to avoid false positives
    else:
        sigma_extreme = 3.5
        sigma_moderate = 1.5
        consecutive_required = 3
    
    # Additional adjustment for high natural jitter
    if coefficient_of_variation > 0.5:  # >50% jitter
        sigma_extreme += 0.5
        consecutive_required += 1
    
    return {
        'sigma_extreme': sigma_extreme,
        'sigma_moderate': sigma_moderate, 
        'consecutive_required': consecutive_required
    }
```

### C.3 Performance Impact Analysis

**Question:** Does per-CAN-ID tuning add overhead?

**Answer:** Essentially zero (0-2 nanoseconds per message).

**Current implementation:**
```python
# Every message already accesses per-CAN-ID values
expected = rule.expected_interval       # Per-CAN-ID (already loaded)
variance = rule.interval_variance       # Per-CAN-ID (already loaded)
extreme_threshold = variance * 3        # Fixed multiplier
```

**Per-CAN-ID implementation:**
```python
# Just load a different value from the rule
expected = rule.expected_interval       # Per-CAN-ID
variance = rule.interval_variance       # Per-CAN-ID  
sigma_multiplier = rule.sigma_extreme   # Per-CAN-ID (pre-computed)
extreme_threshold = variance * sigma_multiplier
```

**Added operations:** 1 attribute access = ~3 CPU cycles = ~1 nanosecond

**Performance at 7,000 msg/s:**
```
Current: 7,000 √ó 20ns = 140 microseconds/second = 0.014% CPU
Per-CAN-ID: 7,000 √ó 21ns = 147 microseconds/second = 0.015% CPU
Difference: 7 microseconds/second = NEGLIGIBLE
```

**Why it's free:**
- Values pre-computed during rule generation (one-time cost)
- Stored in YAML config (no runtime calculation)
- Same memory access pattern as existing code
- No additional branches or loops

**Comparison to expensive operations:**
- Rule iteration: ~100x more expensive
- History management: ~50x more expensive  
- Alert logging: ~100,000x more expensive

### C.4 Implementation Strategy

**Phase 1: Enhance Rule Generation** (Recommended Next Step)

Modify `scripts/generate_rules_from_baseline.py`:

```python
def generate_timing_rule(can_id, stats, total_duration):
    """Generate timing rule with adaptive thresholds."""
    
    # Calculate characteristics
    message_count = stats['count']
    message_rate = message_count / total_duration
    mean_interval = stats['mean']
    std_dev = stats['std']
    cv = std_dev / mean_interval  # Coefficient of variation
    
    # Adaptive threshold selection
    if message_rate > 50:
        sigma_extreme = 2.5 if cv < 0.5 else 3.0
        consecutive_required = 5
        category = "high-traffic"
    elif message_rate > 10:
        sigma_extreme = 3.0 if cv < 0.5 else 3.5
        consecutive_required = 4
        category = "medium-traffic"
    else:
        sigma_extreme = 3.5 if cv < 0.5 else 4.0
        consecutive_required = 3
        category = "low-traffic"
    
    return {
        'name': f'Timing Anomaly - CAN ID 0x{can_id:03X} ({category})',
        'can_id': can_id,
        'expected_interval': mean_interval,
        'interval_variance': std_dev,  # 1-sigma
        'sigma_extreme': sigma_extreme,
        'consecutive_required': consecutive_required,
        'check_timing': True,
        'severity': 'MEDIUM',
        'description': (
            f'Message timing deviates from baseline '
            f'(learned from {message_count:,} messages, '
            f'rate={message_rate:.1f} msg/s, jitter={cv*100:.1f}%)'
        )
    }
```

**Generated rule example:**
```yaml
# High-traffic CAN ID with low jitter - tight thresholds
- name: Timing Anomaly - CAN ID 0x130 (high-traffic)
  can_id: 304
  expected_interval: 19.92
  interval_variance: 2.43      # 1-sigma
  sigma_extreme: 2.5            # 2.5-sigma for extreme detection
  consecutive_required: 5       # Require 5 consecutive violations
  check_timing: true
  severity: MEDIUM
  description: Message timing deviates from baseline (learned from 97,632 messages, rate=50.2 msg/s, jitter=12.2%)

# High-traffic CAN ID with high jitter - moderate thresholds  
- name: Timing Anomaly - CAN ID 0x1E9 (high-traffic)
  can_id: 489
  expected_interval: 10.92
  interval_variance: 5.81      # 1-sigma
  sigma_extreme: 3.0            # 3.0-sigma (adjusted for jitter)
  consecutive_required: 6       # Extra filtering for jitter
  check_timing: true
  severity: MEDIUM
  description: Message timing deviates from baseline (learned from 123,687 messages, rate=91.5 msg/s, jitter=53.2%)

# Low-traffic CAN ID - loose thresholds
- name: Timing Anomaly - CAN ID 0x771 (low-traffic)
  can_id: 1905
  expected_interval: 606.2
  interval_variance: 304.2     # 1-sigma
  sigma_extreme: 3.5            # 3.5-sigma (fewer samples)
  consecutive_required: 3       # Lower requirement (slower traffic)
  check_timing: true
  severity: MEDIUM
  description: Message timing deviates from baseline (learned from 3,208 messages, rate=1.6 msg/s, jitter=50.2%)
```

**Phase 2: Update Detection Engine**

Modify `src/detection/rule_engine.py`:

```python
def _check_timing_violation(self, rule, can_id, timestamp):
    """Hybrid detection with per-CAN-ID adaptive thresholds."""
    
    current_interval = timing_history[-1]
    expected = rule.expected_interval
    variance_1sigma = rule.interval_variance
    
    # Use per-CAN-ID sigma multiplier (with fallback to default)
    sigma_extreme = getattr(rule, 'sigma_extreme', 3.0)
    consecutive_required = getattr(rule, 'consecutive_required', 5)
    
    variance_extreme = variance_1sigma * sigma_extreme
    
    # Tier 1: Extreme violations (per-CAN-ID threshold)
    extreme_min = max(0, expected - variance_extreme)
    extreme_max = expected + variance_extreme
    
    if current_interval < extreme_min or current_interval > extreme_max:
        return True  # Immediate alert
    
    # Tier 2: Consecutive violations (per-CAN-ID count)
    if len(timing_history) >= consecutive_required:
        moderate_min = max(0, expected - variance_1sigma)
        moderate_max = expected + variance_1sigma
        
        recent_intervals = timing_history[-consecutive_required:]
        violations = sum(1 for i in recent_intervals 
                        if i < moderate_min or i > moderate_max)
        
        if violations >= consecutive_required:
            return True  # Sustained pattern
    
    return False
```

### C.5 Expected Improvements

**Predicted Results with Adaptive Thresholds:**

| CAN ID Type | Current FPR | Adaptive FPR | Recall |
|-------------|-------------|--------------|--------|
| High-traffic, low-jitter (0x130) | 15% | **2%** | 99%+ |
| High-traffic, high-jitter (0x1E9) | 45% | **8%** | 98%+ |
| Medium-traffic (0x160) | 25% | **3%** | 99%+ |
| Low-traffic (0x771) | 35% | **5%** | 95%+ |
| **Overall Average** | **40%** | **4-5%** | **98%+** |

**Why this works:**
- ‚úÖ **Tighter thresholds** for low-jitter CAN IDs (reduce FPR from 15% ‚Üí 2%)
- ‚úÖ **Looser thresholds** for high-jitter CAN IDs (reduce FPR from 45% ‚Üí 8%)
- ‚úÖ **Traffic-aware requirements** (5 consecutive for fast, 3 for slow)
- ‚úÖ **Maintains high recall** (98%+ across all CAN ID types)

### C.6 Why Pre-Computed vs Runtime Calculation

**Option A: Pre-Computed (Recommended)**
```yaml
# Values calculated once during rule generation
sigma_extreme: 2.5
consecutive_required: 5
```
- ‚úÖ Zero runtime cost
- ‚úÖ Explicit and tunable
- ‚úÖ Easy to debug (values visible in config)
- ‚úÖ Supports manual overrides

**Option B: Runtime Calculation**
```python
# Calculate every message
if rule.message_rate > 50:
    sigma_extreme = 2.5
```
- ‚ùå 2-3 nanoseconds overhead (still negligible)
- ‚ùå Harder to debug (values invisible)
- ‚ùå Can't manually override per-CAN-ID
- ‚ùå No benefit over pre-computed

**Decision: Use pre-computed approach** for better debuggability and zero runtime cost.

### C.7 Implementation Priority

**Rationale for implementing adaptive thresholds:**

1. **Critical for production:** 40% FPR is unusable, 4-5% is acceptable
2. **Zero performance cost:** No measurable overhead at 7,000 msg/s
3. **Research-validated:** Ming et al. (2023) uses adaptive thresholds successfully
4. **Enables Stage 1 of 7K architecture:** Need <5% FPR for cycle filter

**Effort estimate:**
- Modify `generate_rules_from_baseline.py`: 30-45 minutes
- Update `rule_engine.py`: 15-20 minutes
- Regenerate rules: 5 minutes
- Testing and validation: 20-30 minutes
- **Total: ~1.5-2 hours**

**Expected ROI:**
- FPR reduction: 40% ‚Üí 4-5% (10x improvement)
- Enables 7K architecture Stage 1
- Production-ready detection capability

---

## Appendix D: Implementation Results (December 9, 2025)

### D.1 Implementation Completed

**Changes Made:**

1. **Modified `scripts/generate_rules_from_baseline.py`** (Lines 280-340)
   - Added adaptive threshold calculation based on traffic characteristics
   - Traffic categories: high (>50 msg/s), medium (10-50), low (<10)
   - Jitter adjustment: adds 0.3œÉ for coefficient of variation >50%
   - Base sigma values: 1.5-2.5 (adjusted from initial 2.5-3.5)
   - Consecutive violation counts: 3-5 based on traffic rate

2. **Updated `src/detection/rule_engine.py`**
   - Added fields to `DetectionRule` dataclass:
     * `sigma_extreme`: Per-CAN-ID multiplier for Tier 1 extreme detection
     * `consecutive_required`: Per-CAN-ID count for Tier 2 sustained detection
   - Modified `_check_timing_violation()` to use adaptive parameters
   - Implemented "N out of M" window approach: checks N violations in (N+2) messages
   - Allows 1-2 normal messages interspersed during attack without breaking detection

3. **Generated `config/rules_adaptive.yaml`**
   - 84 timing/frequency rules with per-CAN-ID adaptive thresholds
   - Example for high-traffic, high-jitter (0x1E9): sigma_extreme=1.8, consecutive=4
   - Example for low-traffic, high-jitter (0x771): sigma_extreme=2.8, consecutive=4

### D.2 Testing Results

**Test 1: Interval Attacks (interval-1.csv - 634K messages)**

```
Configuration: rules_adaptive.yaml
- High-traffic: sigma_extreme=1.5-1.8, consecutive=3-4
- Medium-traffic: sigma_extreme=1.7-2.0, consecutive=4
- Low-traffic: sigma_extreme=2.2-2.8, consecutive=3-4

Results:
- Total Alerts: 353,520
- True Positives: 15,125 (100% of attacks caught)
- False Positives: 338,395
- Recall: 100.00% ‚úì
- Precision: 4.28%
- FPR: 54.66% ‚ùå (vs target <5%)
```

**Test 2: Attack-Free Data (attack-free-1.csv - 1.95M messages)**

```
Configuration: rules_adaptive.yaml (same as above)

Results:
- Total Alerts: 1,824,931
- False Positives: 1,824,931
- FPR: 93.45% ‚ùå (unacceptable)
```

**Test 3: Fixed 1-Sigma Baseline (for comparison)**

```
Configuration: rules_timing_1sigma.yaml
- All CAN IDs: sigma_extreme=3.0 (implicit), consecutive=5 (window)

Results on interval-1.csv:
- Recall: 100.00% ‚úì
- FPR: 27.66% (vs 54.66% with adaptive)

Results on attack-free-1.csv:
- FPR: 40.54% (vs 93.45% with adaptive)
```

### D.3 Analysis and Root Cause

**Issue:** Adaptive thresholds with sigma_extreme=1.5-1.8 are TOO AGGRESSIVE.

**Why this happened:**
1. Started with 3-sigma for extreme detection ‚Üí missed attacks (0% recall)
2. Reduced to 2-sigma ‚Üí still missed attacks
3. Reduced to 1.5-1.8 sigma ‚Üí catches attacks BUT too many false positives

**The fundamental challenge:**
- **Interval timing attacks are sophisticated** - designed to fall in 1.5-2œÉ range
- For CAN ID 0x1E9: Normal=10.92ms, Attack=20ms (1.6œÉ away from mean)
- Any threshold <1.6œÉ will catch attack but also catch significant normal variation
- Theoretical FPR at 1.5œÉ: ~13%, at 1.8œÉ: ~7% (single violations)
- Observed FPR: 54-93% because "N out of M" window catches sustained patterns

**Key Discovery:**
Attack messages are interspersed with normal messages:
```
Pattern in interval-1.csv:
- 40 attack messages
- 1 normal message (breaks consecutive count!)
- 40 attack messages  
- 1 normal message
- (repeats)
```

This is why "N out of M" window was needed - pure consecutive checking would fail. But the window also increases FPR on normal traffic with bursts.

### D.4 Path Forward: Tuning Recommendations

**Option 1: Increase sigma_extreme (Conservative)**
```python
# Recommended values for production:
if frequency > 50:
    sigma_extreme_base = 2.2  # Was 1.5
    consecutive_base = 4
elif frequency > 10:
    sigma_extreme_base = 2.5  # Was 1.7
    consecutive_base = 4
else:
    sigma_extreme_base = 3.0  # Was 2.2
    consecutive_base = 3
```

**Expected results:**
- Recall: 90-95% (may miss some subtle attacks)
- FPR: 5-15% (acceptable for production)

**Option 2: Tighten "N out of M" window**
```python
# Current: N violations out of (N+2) messages
# Proposed: N violations out of N messages (pure consecutive)
window_size = consecutive_required  # Instead of consecutive_required + 2
```

**Expected results:**
- FPR: Reduced by 30-40%
- Recall: May drop to 85-90% if attacks have interruptions

**Option 3: Hybrid - Adjust both**
```python
# Moderate sigma with pure consecutive
sigma_extreme: 2.0-2.5 (was 1.5-1.8)
window: N out of N (was N out of N+2)
```

**Expected results:**
- Recall: 90-95%
- FPR: 8-15%

**Option 4: Add Tier 3 - Payload Analysis**
```python
# Timing violation + Payload repetition = High confidence
if timing_violation and payload_repeated:
    return True  # Very high confidence
```

**Expected results:**
- Recall: 95-99% (timing OR payload catches most attacks)
- FPR: 3-5% (requires BOTH timing AND payload for alert)

### D.5 Comparison: Fixed vs Adaptive (Current State)

| Metric | Fixed 1œÉ | Adaptive (Current) | Adaptive (Target) |
|--------|----------|-------------------|-------------------|
| **Recall (interval-1)** | 100% | 100% | 90-95% |
| **FPR (interval-1)** | 27.66% | 54.66% | 5-10% |
| **FPR (attack-free)** | 40.54% | 93.45% | 3-8% |
| **sigma_extreme** | 3.0 (implicit) | 1.5-2.8 | 2.0-3.0 |
| **Consecutive** | 5 (window 7) | 3-4 (window 5-6) | 4-5 (window 4-5) |
| **Status** | Working baseline | Too aggressive | Need tuning |

### D.6 Lessons Learned

1. **Sophisticated attacks are hard to detect with timing alone**
   - Interval attacks fall in 1.5-2œÉ range (within statistical noise)
   - Cannot achieve both <5% FPR and >95% recall with timing only
   - Need additional signals (payload, frequency, sequence)

2. **Per-CAN-ID adaptation is essential but insufficient**
   - Different CAN IDs DO need different thresholds (proven)
   - But aggressive thresholds catch normal variation even with adaptation
   - Zero performance overhead makes it "free" to implement

3. **Window-based detection has tradeoffs**
   - "N out of M" catches attacks with interruptions ‚úì
   - But also catches normal traffic bursts ‚úó
   - Need careful tuning of window size

4. **Test data matters**
   - interval-1.csv: 27% FPR (one vehicle/scenario)
   - attack-free-1.csv: 40-93% FPR (different vehicle/conditions)
   - Vehicle-specific tuning may be necessary

5. **Multi-tier detection works**
   - Tier 1 (extreme): Catches obvious attacks immediately
   - Tier 2 (sustained): Catches subtle attacks with pattern
   - Tier 3 (not implemented): Would combine timing + payload

---

## Appendix E: Next Steps

### Completed (December 8-9, 2025)
1. ‚úÖ Document hybrid approach (Appendix C)
2. ‚úÖ Implement hybrid detection in rule_engine.py (Tier 1 + Tier 2)
3. ‚úÖ Test initial approach (99.69% recall, 25% FPR with fixed 3œÉ + 5 consecutive)
4. ‚úÖ Identify FPR issue (40% on attack-free data with fixed thresholds)
5. ‚úÖ Document adaptive threshold solution (Appendix C)
6. ‚úÖ Implement adaptive per-CAN-ID thresholds (Appendix D)
   - Modified `generate_rules_from_baseline.py` with traffic-based adaptation
   - Added `sigma_extreme` and `consecutive_required` to DetectionRule dataclass
   - Updated `rule_engine.py` to use per-CAN-ID values with "N out of M" window
   - Generated rules with adaptive thresholds
7. ‚úÖ Validate implementation (Appendix D.2)
   - interval-1.csv: 100% recall, 54.66% FPR (too aggressive)
   - attack-free-1.csv: 93.45% FPR (too aggressive)
   - Identified root cause: sigma_extreme=1.5-1.8 is too tight for 1.5-2œÉ attacks
8. ‚úÖ Document results and analysis (Appendix D.3-D.6)

### Immediate Next Steps (Threshold Tuning)
1. **Tune adaptive thresholds** (30 minutes)
   - Increase sigma_extreme: 1.5-1.8 ‚Üí 2.0-2.5
   - Test Option 3 (Hybrid): moderate sigma + tighter window
   - Target: 90-95% recall, 5-10% FPR
   
2. **Validate tuned thresholds** (20 minutes)
   - Test on interval-1.csv (target: 90-95% recall, 5-10% FPR)
   - Test on attack-free-1.csv (target: 3-8% FPR)
   - Compare to fixed 1œÉ baseline

3. **Document final results** (10 minutes)
   - Update Appendix D with tuned metrics
   - Create final recommendation

### Short Term (Next Session)
1. **Test on other attack types** (1 hour)
   - DoS attacks (DoS-1.csv) - expect Tier 1 extreme violation detection
   - Fuzzing attacks (fuzzing-1.csv) - timing may not be effective
   - Document coverage per attack type

2. **Implement Tier 3 (Payload Analysis)** (2-3 hours) - Optional
   - Add payload repetition detection
   - Combine timing + payload for high-confidence alerts
   - Expected: 95-99% recall, 3-5% FPR

3. **Integrate with 7K Architecture** (4-6 hours)
   - Package as Stage 1 (Cycle Filter)
   - Test throughput at 7,000 msg/s
   - Measure actual CPU usage and latency

### Medium Term (Production Readiness)
1. **Per-vehicle tuning framework** (3-4 hours)
   - Allow learning period per vehicle
   - Auto-generate vehicle-specific rules
   - Store baseline per VIN

2. **Performance optimization** (2-3 hours)
   - Implement CAN ID indexing for rule lookup
   - Profile and optimize hot paths
   - Validate 7K msg/s sustained throughput

3. **Monitoring and telemetry** (2-3 hours)
   - Add FPR tracking
   - Detection latency metrics
   - Rule effectiveness per CAN ID

### Integration with 7K Architecture
1. Implement Stage 1 (Cycle Filter) with hybrid timing detection
2. Implement Stage 2 (Rule Engine) optimizations (CAN ID indexing)
3. Reduce ML to 5-15 estimators for Stage 3
4. End-to-end testing at 7,000 msg/s

### Production Hardening
1. Add configuration options for tuning (N, sigma multipliers)
2. Implement adaptive thresholding (learn from deployment data)
3. Add telemetry (detection latency, FPR monitoring)
4. Performance profiling and optimization

---

## References

- Ming, Z., et al. (2023). "Threshold-Adaptive Message Cycle Detection for CAN Intrusion Detection Systems"
- Yu, J., et al. (2023). "TCE-IDS: A Cross-Check Filter Architecture for Intrusion Detection"
- BUILD_PLAN_7000_MSG_SEC.md - 7K msg/s architecture design
- RULE_GENERATION_SUMMARY.md - Baseline rule generation methodology
